{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging - Lexicon and Rule Based Taggers\n",
    "\n",
    "Let's look at the two most basic tagging techniques - lexicon based (or unigram) and rule-based. \n",
    "\n",
    "In this guided exercise, you will explore the WSJ (wall street journal) POS-tagged corpus that comes with NLTK and build a lexicon and rule-based tagger using this corpus as the tarining data. \n",
    "\n",
    "This exercise is divided into the following sections:\n",
    "1. Reading and understanding the tagged dataset\n",
    "2. Exploratory analysis\n",
    "3. Lexicon and rule-based models:\n",
    "    - Creating and evaluating a lexicon POS tagger\n",
    "    - Creating and evaluating a rule-based POS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading and understanding the tagged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\gopal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\gopal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping help\\tagsets.zip.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint, time\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('tagsets')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the Treebank tagged sentences\n",
    "wsj = list(nltk.corpus.treebank.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[('Pierre', 'NNP'),\n",
       "  ('Vinken', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('61', 'CD'),\n",
       "  ('years', 'NNS'),\n",
       "  ('old', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('will', 'MD'),\n",
       "  ('join', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('board', 'NN'),\n",
       "  ('as', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('nonexecutive', 'JJ'),\n",
       "  ('director', 'NN'),\n",
       "  ('Nov.', 'NNP'),\n",
       "  ('29', 'CD'),\n",
       "  ('.', '.')],\n",
       " [('Mr.', 'NNP'),\n",
       "  ('Vinken', 'NNP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('chairman', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('Elsevier', 'NNP'),\n",
       "  ('N.V.', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('the', 'DT'),\n",
       "  ('Dutch', 'NNP'),\n",
       "  ('publishing', 'VBG'),\n",
       "  ('group', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Rudolph', 'NNP'),\n",
       "  ('Agnew', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('55', 'CD'),\n",
       "  ('years', 'NNS'),\n",
       "  ('old', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('former', 'JJ'),\n",
       "  ('chairman', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('Consolidated', 'NNP'),\n",
       "  ('Gold', 'NNP'),\n",
       "  ('Fields', 'NNP'),\n",
       "  ('PLC', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('was', 'VBD'),\n",
       "  ('named', 'VBN'),\n",
       "  ('*-1', '-NONE-'),\n",
       "  ('a', 'DT'),\n",
       "  ('nonexecutive', 'JJ'),\n",
       "  ('director', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('this', 'DT'),\n",
       "  ('British', 'JJ'),\n",
       "  ('industrial', 'JJ'),\n",
       "  ('conglomerate', 'NN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# samples: Each sentence is a list of (word, pos) tuples\n",
    "wsj[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the list mentioned above, each element of the list is a sentence. Also, note that each sentence ends with a full stop '.' whose POS tag is also a '.'. Thus, the POS tag '.' demarcates the end of a sentence.\n",
    "\n",
    "Also, we do not need the corpus to be segmented into sentences, but can rather use a list of (word, tag) tuples. Let's convert the list into a (word, tag) tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100676\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT')]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# converting the list of sents to a list of (word, pos tag) tuples\n",
    "tagged_words = [tup for sent in wsj for tup in sent]\n",
    "print(len(tagged_words))\n",
    "tagged_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of about 100676 (word, tag) tuples. Let's now do some exploratory analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Analysis\n",
    "\n",
    "Let's now conduct some basic exploratory analysis to understand the tagged corpus. To start with, let's ask some simple questions:\n",
    "1. How many unique tags are there in the corpus? \n",
    "2. Which is the most frequent tag in the corpus?\n",
    "3. Which tag is most commonly assigned to the following words:\n",
    "    - \"bank\"\n",
    "    - \"executive\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# question 1: Find the number of unique POS tags in the corpus\n",
    "# you can use the set() function on the list of tags to get a unique set of tags, \n",
    "# and compute its length\n",
    "tags = [pair[1] for pair in tagged_words]\n",
    "unique_tags = set(tags)\n",
    "len(unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({'NNP': 9410,\n",
       "         ',': 4886,\n",
       "         'CD': 3546,\n",
       "         'NNS': 6047,\n",
       "         'JJ': 5834,\n",
       "         'MD': 927,\n",
       "         'VB': 2554,\n",
       "         'DT': 8165,\n",
       "         'NN': 13166,\n",
       "         'IN': 9857,\n",
       "         '.': 3874,\n",
       "         'VBZ': 2125,\n",
       "         'VBG': 1460,\n",
       "         'CC': 2265,\n",
       "         'VBD': 3043,\n",
       "         'VBN': 2134,\n",
       "         '-NONE-': 6592,\n",
       "         'RB': 2822,\n",
       "         'TO': 2179,\n",
       "         'PRP': 1716,\n",
       "         'RBR': 136,\n",
       "         'WDT': 445,\n",
       "         'VBP': 1321,\n",
       "         'RP': 216,\n",
       "         'PRP$': 766,\n",
       "         'JJS': 182,\n",
       "         'POS': 824,\n",
       "         '``': 712,\n",
       "         'EX': 88,\n",
       "         \"''\": 694,\n",
       "         'WP': 241,\n",
       "         ':': 563,\n",
       "         'JJR': 381,\n",
       "         'WRB': 178,\n",
       "         '$': 724,\n",
       "         'NNPS': 244,\n",
       "         'WP$': 14,\n",
       "         '-LRB-': 120,\n",
       "         '-RRB-': 126,\n",
       "         'PDT': 27,\n",
       "         'RBS': 35,\n",
       "         'FW': 4,\n",
       "         'UH': 3,\n",
       "         'SYM': 1,\n",
       "         'LS': 13,\n",
       "         '#': 16})"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "# question 2: Which is the most frequent tag in the corpus\n",
    "# to count the frequency of elements in a list, the Counter() class from collections\n",
    "# module is very useful, as shown below\n",
    "\n",
    "from collections import Counter\n",
    "tag_counts = Counter(tags)\n",
    "tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('NN', 13166), ('IN', 9857), ('NNP', 9410), ('DT', 8165), ('-NONE-', 6592)]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# the most common tags can be seen using the most_common() method of Counter\n",
    "tag_counts.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, NN is the most common tag followed by IN, NNP, DT, -NONE- etc. You can read the exhaustive list of tags using the NLTK documentation as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "$: dollar\n    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n'': closing quotation mark\n    ' ''\n(: opening parenthesis\n    ( [ {\n): closing parenthesis\n    ) ] }\n,: comma\n    ,\n--: dash\n    --\n.: sentence terminator\n    . ! ?\n:: colon or ellipsis\n    : ; ...\nCC: conjunction, coordinating\n    & 'n and both but either et for less minus neither nor or plus so\n    therefore times v. versus vs. whether yet\nCD: numeral, cardinal\n    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n    fifteen 271,124 dozen quintillion DM2,000 ...\nDT: determiner\n    all an another any both del each either every half la many much nary\n    neither no some such that the them these this those\nEX: existential there\n    there\nFW: foreign word\n    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n    terram fiche oui corporis ...\nIN: preposition or conjunction, subordinating\n    astride among uppon whether out inside pro despite on by throughout\n    below within for towards near behind atop around if like until below\n    next into if beside ...\nJJ: adjective or numeral, ordinal\n    third ill-mannered pre-war regrettable oiled calamitous first separable\n    ectoplasmic battery-powered participatory fourth still-to-be-named\n    multilingual multi-disciplinary ...\nJJR: adjective, comparative\n    bleaker braver breezier briefer brighter brisker broader bumper busier\n    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n    cozier creamier crunchier cuter ...\nJJS: adjective, superlative\n    calmest cheapest choicest classiest cleanest clearest closest commonest\n    corniest costliest crassest creepiest crudest cutest darkest deadliest\n    dearest deepest densest dinkiest ...\nLS: list item marker\n    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n    SP-44007 Second Third Three Two * a b c d first five four one six three\n    two\nMD: modal auxiliary\n    can cannot could couldn't dare may might must need ought shall should\n    shouldn't will would\nNN: noun, common, singular or mass\n    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n    investment slide humour falloff slick wind hyena override subhumanity\n    machinist ...\nNNP: noun, proper, singular\n    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n    Shannon A.K.C. Meltex Liverpool ...\nNNPS: noun, proper, plural\n    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n    Apache Apaches Apocrypha ...\nNNS: noun, common, plural\n    undergraduates scotches bric-a-brac products bodyguards facets coasts\n    divestitures storehouses designs clubs fragrances averages\n    subjectivists apprehensions muses factory-jobs ...\nPDT: pre-determiner\n    all both half many quite such sure this\nPOS: genitive marker\n    ' 's\nPRP: pronoun, personal\n    hers herself him himself hisself it itself me myself one oneself ours\n    ourselves ownself self she thee theirs them themselves they thou thy us\nPRP$: pronoun, possessive\n    her his mine my our ours their thy your\nRB: adverb\n    occasionally unabatingly maddeningly adventurously professedly\n    stirringly prominently technologically magisterially predominately\n    swiftly fiscally pitilessly ...\nRBR: adverb, comparative\n    further gloomier grander graver greater grimmer harder harsher\n    healthier heavier higher however larger later leaner lengthier less-\n    perfectly lesser lonelier longer louder lower more ...\nRBS: adverb, superlative\n    best biggest bluntest earliest farthest first furthest hardest\n    heartiest highest largest least less most nearest second tightest worst\nRP: particle\n    aboard about across along apart around aside at away back before behind\n    by crop down ever fast for forth from go high i.e. in into just later\n    low more off on open out over per pie raising start teeth that through\n    under unto up up-pp upon whole with you\nSYM: symbol\n    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\nTO: \"to\" as preposition or infinitive marker\n    to\nUH: interjection\n    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n    man baby diddle hush sonuvabitch ...\nVB: verb, base form\n    ask assemble assess assign assume atone attention avoid bake balkanize\n    bank begin behold believe bend benefit bevel beware bless boil bomb\n    boost brace break bring broil brush build ...\nVBD: verb, past tense\n    dipped pleaded swiped regummed soaked tidied convened halted registered\n    cushioned exacted snubbed strode aimed adopted belied figgered\n    speculated wore appreciated contemplated ...\nVBG: verb, present participle or gerund\n    telegraphing stirring focusing angering judging stalling lactating\n    hankerin' alleging veering capping approaching traveling besieging\n    encrypting interrupting erasing wincing ...\nVBN: verb, past participle\n    multihulled dilapidated aerosolized chaired languished panelized used\n    experimented flourished imitated reunifed factored condensed sheared\n    unsettled primed dubbed desired ...\nVBP: verb, present tense, not 3rd person singular\n    predominate wrap resort sue twist spill cure lengthen brush terminate\n    appear tend stray glisten obtain comprise detest tease attract\n    emphasize mold postpone sever return wag ...\nVBZ: verb, present tense, 3rd person singular\n    bases reconstructs marks mixes displeases seals carps weaves snatches\n    slumps stretches authorizes smolders pictures emerges stockpiles\n    seduces fizzes uses bolsters slaps speaks pleads ...\nWDT: WH-determiner\n    that what whatever which whichever\nWP: WH-pronoun\n    that what whatever whatsoever which who whom whosoever\nWP$: WH-pronoun, possessive\n    whose\nWRB: Wh-adverb\n    how however whence whenever where whereby whereever wherein whereof why\n``: opening quotation mark\n    ` ``\n"
     ]
    }
   ],
   "source": [
    "# list of POS tags in NLTK\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({('bank', 'NN'): 38, ('Bank', 'NNP'): 32})"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# question 3: Which tag is most commonly assigned to the word w.\n",
    "bank = [pair for pair in tagged_words if pair[0].lower() == 'bank']\n",
    "Counter(bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Counter({('executive', 'NN'): 40, ('executive', 'JJ'): 28})"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "# question 3: Which tag is most commonly assigned to the word w.\n",
    "executive = [pair for pair in tagged_words if pair[0].lower() == 'executive']\n",
    "Counter(executive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Analysis Contd.\n",
    "\n",
    "Until now, we were looking at the frequency of tags assigned to particular words, which is the basic idea used by lexicon or unigram taggers. Let's now try observing some rules which can potentially be used for POS tagging. \n",
    "\n",
    "To start with, let's see if the following questions reveal something useful:\n",
    "\n",
    "4. What fraction of words with the tag 'VBD' (verb, past tense) end with the letters 'ed'\n",
    "5. What fraction of words with the tag 'VBG' (verb, present participle/gerund) end with the letters 'ing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3881038448899113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('reported', 'VBD'),\n",
       " ('stopped', 'VBD'),\n",
       " ('studied', 'VBD'),\n",
       " ('led', 'VBD'),\n",
       " ('worked', 'VBD'),\n",
       " ('explained', 'VBD'),\n",
       " ('imposed', 'VBD'),\n",
       " ('dumped', 'VBD'),\n",
       " ('poured', 'VBD'),\n",
       " ('mixed', 'VBD'),\n",
       " ('described', 'VBD'),\n",
       " ('ventilated', 'VBD'),\n",
       " ('contracted', 'VBD'),\n",
       " ('continued', 'VBD'),\n",
       " ('eased', 'VBD'),\n",
       " ('ended', 'VBD'),\n",
       " ('lengthened', 'VBD'),\n",
       " ('reached', 'VBD'),\n",
       " ('resigned', 'VBD'),\n",
       " ('approved', 'VBD')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. how many words with the tag 'VBD' (verb, past tense) end with 'ed'\n",
    "past_tense_verbs = [pair for pair in tagged_words if pair[1]=='VBD']\n",
    "ed_verbs = [pair for pair in past_tense_verbs if pair[0].endswith('ed')]\n",
    "print(len(ed_verbs) / len(past_tense_verbs))\n",
    "ed_verbs[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9972602739726028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('publishing', 'VBG'),\n",
       " ('causing', 'VBG'),\n",
       " ('using', 'VBG'),\n",
       " ('talking', 'VBG'),\n",
       " ('having', 'VBG'),\n",
       " ('making', 'VBG'),\n",
       " ('surviving', 'VBG'),\n",
       " ('including', 'VBG'),\n",
       " ('including', 'VBG'),\n",
       " ('according', 'VBG'),\n",
       " ('remaining', 'VBG'),\n",
       " ('according', 'VBG'),\n",
       " ('declining', 'VBG'),\n",
       " ('rising', 'VBG'),\n",
       " ('yielding', 'VBG'),\n",
       " ('waiving', 'VBG'),\n",
       " ('holding', 'VBG'),\n",
       " ('holding', 'VBG'),\n",
       " ('cutting', 'VBG'),\n",
       " ('manufacturing', 'VBG')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. how many words with the tag 'VBG' end with 'ing'\n",
    "participle_verbs = [pair for pair in tagged_words if pair[1]=='VBG']\n",
    "ing_verbs = [pair for pair in participle_verbs if pair[0].endswith('ing')]\n",
    "print(len(ing_verbs) / len(participle_verbs))\n",
    "ing_verbs[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Analysis Continued\n",
    "\n",
    "Let's now try observing some tag patterns using the fact the some tags are more likely to apper after certain other tags. For e.g. most nouns NN are usually followed by determiners DT (\"The/DT constitution/NN\"), adjectives JJ usually precede a noun NN (\" A large/JJ building/NN\"), etc. \n",
    "\n",
    "Try answering the following questions:\n",
    "1. What fraction of adjectives JJ are followed by a noun NN? \n",
    "2. What fraction of determiners DT are followed by a noun NN?\n",
    "3. What fraction of modals MD are followed by a verb VB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5834\n",
      "2611\n",
      "0.4475488515598217\n"
     ]
    }
   ],
   "source": [
    "# question: what fraction of adjectives JJ are followed by a noun NN\n",
    "\n",
    "# create a list of all tags (without the words)\n",
    "tags = [pair[1] for pair in tagged_words]\n",
    "\n",
    "# create a list of JJ tags\n",
    "jj_tags = [t for t in tags if t == 'JJ']\n",
    "\n",
    "# create a list of (JJ, NN) tags\n",
    "jj_nn_tags = [(t, tags[index+1]) for index, t in enumerate(tags) \n",
    "              if t=='JJ' and tags[index+1]=='NN']\n",
    "\n",
    "print(len(jj_tags))\n",
    "print(len(jj_nn_tags))\n",
    "print(len(jj_nn_tags) / len(jj_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8165\n",
      "3844\n",
      "0.470789957134109\n"
     ]
    }
   ],
   "source": [
    "# question: what fraction of determiners DT are followed by a noun NN\n",
    "dt_tags = [t for t in tags if t == 'DT']\n",
    "dt_nn_tags = [(t, tags[index+1]) for index, t in enumerate(tags) \n",
    "              if t=='DT' and tags[index+1]=='NN']\n",
    "\n",
    "print(len(dt_tags))\n",
    "print(len(dt_nn_tags))\n",
    "print(len(dt_nn_tags) / len(dt_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "927\n",
      "756\n",
      "0.8155339805825242\n"
     ]
    }
   ],
   "source": [
    "# question: what fraction of modals MD are followed by a verb VB?\n",
    "md_tags = [t for t in tags if t == 'MD']\n",
    "md_vb_tags = [(t, tags[index+1]) for index, t in enumerate(tags) \n",
    "              if t=='MD' and tags[index+1]=='VB']\n",
    "\n",
    "print(len(md_tags))\n",
    "print(len(md_vb_tags))\n",
    "print(len(md_vb_tags) / len(md_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we see that the probability of certain tags appearing after certain other tags is quite high, and this fact can be used to build quite efficient POS tagging algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lexicon and Rule-Based Models for POS Tagging\n",
    "\n",
    "Let's now see lexicon and rule-based models for POS tagging. We'll first split the corpus into training and test sets and then use built-in NLTK taggers. \n",
    "\n",
    "### 3.1 Splitting into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2739\n1175\n[[('As', 'IN'), ('an', 'DT'), ('actor', 'NN'), (',', ','), ('Charles', 'NNP'), ('Lane', 'NNP'), ('is', 'VBZ'), (\"n't\", 'RB'), ('the', 'DT'), ('inheritor', 'NN'), ('of', 'IN'), ('Charlie', 'NNP'), ('Chaplin', 'NNP'), (\"'s\", 'POS'), ('spirit', 'NN'), ('.', '.')], [('With', 'IN'), ('the', 'DT'), ('limit', 'NN'), ('in', 'IN'), ('effect', 'NN'), (',', ','), ('members', 'NNS'), ('would', 'MD'), ('be', 'VB'), ('able', 'JJ'), ('*-1', '-NONE-'), ('to', 'TO'), ('execute', 'VB'), ('trades', 'NNS'), ('at', 'IN'), ('the', 'DT'), ('limit', 'NN'), ('price', 'NN'), ('or', 'CC'), ('at', 'IN'), ('higher', 'JJR'), ('prices', 'NNS'), (',', ','), ('but', 'CC'), ('not', 'RB'), ('below', 'IN'), ('it', 'PRP'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# splitting into train and test\n",
    "random.seed(1234)\n",
    "train_set, test_set = train_test_split(wsj, test_size=0.3)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "print(train_set[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Lexicon (Unigram) Tagger\n",
    "\n",
    "Let's now try training a lexicon (or a unigram) tagger which assigns the most commonly assigned tag to a word. \n",
    "\n",
    "In NLTK, the `UnigramTagger()`  can be used to train such a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8735673943917825"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "# Lexicon (or unigram tagger)\n",
    "unigram_tagger = nltk.UnigramTagger(train_set)\n",
    "unigram_tagger.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.14872675628364765"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# Lexicon (or Bigram tagger)\n",
    "Bigram_tagger = nltk.BigramTagger(train_set)\n",
    "Bigram_tagger.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even a simple unigram tagger seems to perform fairly well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Rule-Based (Regular Expression) Tagger\n",
    "\n",
    "Now let's build a rule-based, or regular expression based tagger. In NLTK, the `RegexpTagger()` can be provided with handwritten regular expression patterns, as shown below.\n",
    "\n",
    "In the example below, we specify regexes for gerunds and past tense verbs (as seen above), 3rd singular present verb (creates, moves, makes etc.), modal verbs MD (should, would, could), possesive nouns (partner's, bank's etc.), plural nouns (banks, institutions), cardinal numbers CD and finally, if none of the above rules are applicable to a word, we tag the most frequent tag NN.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify patterns for tagging\n",
    "# example from the NLTK book\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),              # gerund\n",
    "    (r'.*ed$', 'VBD'),               # past tense\n",
    "    (r'.*es$', 'VBZ'),               # 3rd singular present\n",
    "    (r'.*ould$', 'MD'),              # modals\n",
    "    (r'.*\\'s$', 'NN$'),              # possessive nouns\n",
    "    (r'.*s$', 'NNS'),                # plural nouns\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'), # cardinal numbers\n",
    "    (r'.*', 'NN')                    # nouns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "# help(regexp_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21931829474311834"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tagger.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Combining Taggers\n",
    "\n",
    "Let's now try combining the taggers created above. We saw that the rule-based tagger by itself is quite ineffective since we've only written a handful of rules. However, if we could combine the lexicon and the rule-based tagger, we can potentially create a tagger much better than any of the individual ones.\n",
    "\n",
    "NLTK provides a convenient way to combine taggers using the 'backup' argument. In the following code, we create a regex tagger which is used as a backup tagger to the lexicon tagger, i.e. when the tagger is not able to tag using the lexicon (in case of a new word not in the vocabulary), it uses the rule-based tagger. \n",
    "\n",
    "Also, note that the rule-based tagger itself is backed up by the tag 'NN'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9049985093908377"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rule based tagger\n",
    "rule_based_tagger = nltk.RegexpTagger(patterns)\n",
    "\n",
    "# lexicon backed up by the rule-based tagger\n",
    "lexicon_tagger = nltk.UnigramTagger(train_set, backoff=rule_based_tagger)\n",
    "\n",
    "lexicon_tagger.evaluate(test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python383jvsc74a57bd0da6e2f449bd6a25c403a634f7772ee01c1cf6132173f7ff2206f401b82c95e6e",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}